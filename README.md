# gRPC Monitoring Demo with Prometheus & Grafana

This repository demonstrates how to instrument a Go-gRPC server and client with Prometheus metrics, collect containerâ€‘level stats via cAdvisor, and visualize everything in Grafana.

### Why This Project?

* During my recent DevOps work with Prometheus, Loki, Grafana, OpenTelemetry and related tools, I recognized their power and wanted to build a similar monitoring pipeline myself.
* This demo shows an endâ€‘toâ€‘end setup: from instrumenting code to visualizing metrics in Grafana.
* As a Go & gRPC enthusiast, I wanted a clean, reproducible example that anyone can clone and run.

## Project Overview

* **gRPC server**: A Go server exposing a `Monitoring` RPC. Instrumented with Prometheus middleware for request counts, error rates, and latencies.
* **gRPC client**: A Go client that periodically sends correct (`ping`) and incorrect (`wrong`) requests to the server. Exposes its own Prometheus metrics (total, success, and failure counts).
* **cAdvisor**: Collects CPU, memory, disk, and network metrics for all containers.
* **Prometheus**: Scrapes metrics from the gRPC server (`:2001/metrics`), gRPC client (`:2016/metrics`), cAdvisor (`:8080/metrics`), and itself. Runs at `:9099`.
* **Grafana**: Autoâ€‘imports a dashboard that shows both application and container metrics. Accessible at `http://localhost:3005`.

## Repository Structure

```
.
â”œâ”€â”€ certs/                         # mTLS files and instructions (README inside)
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ cmd/main/                  # gRPC client entrypoint with Prometheus metrics
â”‚   â””â”€â”€ internal/
â”‚       â”œâ”€â”€ config/                # Client config loader
â”‚       â”œâ”€â”€ pb/                    # Generated protobuf for monitoring.proto
â”‚       â”œâ”€â”€ security/              # Client TLS credentials loader
â”‚       â””â”€â”€ service/               # Client code (sends ping/wrong periodically)
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ cmd/main/                  # gRPC server entrypoint with Prometheus metrics
â”‚   â””â”€â”€ internal/
â”‚       â”œâ”€â”€ config/                # Server config loader
â”‚       â”œâ”€â”€ pb/                    # Generated protobuf for monitoring.proto
â”‚       â”œâ”€â”€ security/              # Server TLS credentials loader
â”‚       â””â”€â”€ service/               # Service implementation (Monitoring RPC)
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ provisioning/          # Grafana provisioning folder
â”‚   â”‚       â”œâ”€â”€ dashboards/
â”‚   â”‚       â”‚   â”œâ”€â”€ dashboards.yaml
â”‚   â”‚       â”‚   â””â”€â”€ grpc_monitoring_dashboard.json
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml         # Prometheus scrape configuration
â”œâ”€â”€ image/
â”‚   â””â”€â”€ grafana.png                # Example Grafana dashboard screenshot
â”œâ”€â”€ docker-compose.yml             # Orchestrates server, client, cAdvisor, Prometheus, Grafana
â””â”€â”€ README.md                      # (this file)
```

## Prerequisites

* **Docker & DockerÂ Compose** installed on your machine.
* **GoÂ 1.18+** (if you want to rebuild the client/server locally).

## Generate mTLS Certificates

All TLS materials live under`./certs`. To generate them, follow the instructions in that folder:

```
cd certs
# Read certs/README.md and run the commands to generate CA, server, and client certificates.
```

After following those steps, `certs/` should contain:

```
ca.crt.pem
ca.key.pem
client.crt.pem
client.key.pem
server.crt.pem
server.key.pem
server_ext.cnf
client_ext.cnf
```

## Running the Stack

1. **Build and start all services**

   From the repo root:

   ```bash
   docker-compose up -d --build
   ```

   This will launch:

    * `grpc_server`Â (listening onÂ `:50059`Â for gRPC, `:2001/metrics`Â for Prometheus)
    * `grpc_client`Â (sends ping/wrong, exposes `:2016/metrics`)
    * `cadvisor`Â (exposes `:8080/metrics` for container stats)
    * `prometheus`Â (onÂ `:9099`; scrapes server, client, cAdvisor, and itself)
    * `grafana`Â (onÂ `:3005`; autoâ€‘imports the dashboard)

2. **Verify Prometheus targets**

   Open `http://localhost:9099` in your browser. Under **Status â†’ Targets**, you should see four jobs in the â€œUPâ€ state:

    * `grpc_server:2001`
    * `grpc_client:2016`
    * `cadvisor:8080`
    * `prometheus:9090`

3. **View Grafana dashboard**

   Open `http://localhost:3005`. Log in with the default `admin` / `admin` credentials.Â 
   Navigate to **Dashboards â†’ Manage** and select **â€œgRPC & Container Monitoringâ€**. You will see panels for:

    * gRPC server request & error rates, P95/median latency.
    * gRPC client total/success/failed request rates.
    * Container CPU %, memory usage, and filesystem usage for `grpc_server` and `grpc_client`.

   ![Grafana Dashboard](image/grafana.png)

---

## Final words

Modern observability stacks are incredibly powerful and ever-evolving. Iâ€™m excited to continue learning about Prometheus, Grafana, Loki, OpenTelemetry and other tools to build even more robust systems.

This project is meant to help not just me, but also youâ€”so you can follow along, experiment, and build your own monitoring pipelines!

## Further Reading

* [Prometheus](https://prometheus.io/docs/)
* [Grafana](https://grafana.com/docs/)
* [Loki](https://grafana.com/oss/loki/)
* [OpenTelemetry](https://opentelemetry.io/docs/): 
* [cAdvisor](https://github.com/google/cadvisor)

Feel free to explore these official resources to deepen your understanding of modern monitoring and observability tools.

Happy monitoring!ğŸ˜